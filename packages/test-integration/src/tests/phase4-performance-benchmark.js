#!/usr/bin/env node
/**
 * Phase 4.4: Performance Benchmarking
 * Tests response times, cost optimization, and quality metrics
 */

const chalk = require('chalk');
const dotenv = require('dotenv');
const path = require('path');
const { createClient } = require('@supabase/supabase-js');

// Load environment variables
dotenv.config({ path: path.resolve(__dirname, '../../../../.env') });

async function runPerformanceBenchmark() {
  console.log(chalk.bold.blue('\nðŸš€ Phase 4.4: Performance Benchmarking\n'));

  const supabase = createClient(
    process.env.SUPABASE_URL,
    process.env.SUPABASE_SERVICE_ROLE_KEY
  );

  try {
    // Get all model configurations for benchmarking
    const { data: configs } = await supabase
      .from('model_configurations')
      .select('*')
      .in('language', ['python', 'javascript', 'java'])
      .order('test_results->pricing->input');

    console.log(chalk.yellow('ðŸ“Š Benchmarking Models:'));
    console.log(chalk.gray(`   Total configurations: ${configs.length}`));

    // Group by cost tier
    const costTiers = {
      budget: configs.filter(c => {
        const avgCost = ((c.test_results?.pricing?.input || 0) + (c.test_results?.pricing?.output || 0)) / 2;
        return avgCost < 1;
      }),
      standard: configs.filter(c => {
        const avgCost = ((c.test_results?.pricing?.input || 0) + (c.test_results?.pricing?.output || 0)) / 2;
        return avgCost >= 1 && avgCost < 5;
      }),
      premium: configs.filter(c => {
        const avgCost = ((c.test_results?.pricing?.input || 0) + (c.test_results?.pricing?.output || 0)) / 2;
        return avgCost >= 5;
      })
    };

    console.log(chalk.blue('\nðŸ’° Cost Tiers:'));
    console.log(chalk.gray(`   Budget (<$1/1M): ${costTiers.budget.length} models`));
    console.log(chalk.gray(`   Standard ($1-5/1M): ${costTiers.standard.length} models`));
    console.log(chalk.gray(`   Premium (>$5/1M): ${costTiers.premium.length} models`));

    // Benchmark scenarios
    const scenarios = [
      {
        name: 'Quick Security Scan',
        tokens: 1000,
        priority: 'speed',
        acceptableLatency: 5000
      },
      {
        name: 'Full Repository Analysis',
        tokens: 10000,
        priority: 'quality',
        acceptableLatency: 30000
      },
      {
        name: 'Real-time PR Review',
        tokens: 3000,
        priority: 'balanced',
        acceptableLatency: 10000
      }
    ];

    console.log(chalk.yellow('\nðŸ“‹ Benchmark Scenarios:'));
    
    const benchmarkResults = [];

    for (const scenario of scenarios) {
      console.log(chalk.blue(`\n${scenario.name}:`));
      console.log(chalk.gray(`   Token requirement: ${scenario.tokens}`));
      console.log(chalk.gray(`   Priority: ${scenario.priority}`));
      console.log(chalk.gray(`   Max latency: ${scenario.acceptableLatency}ms`));

      // Select optimal model based on scenario
      let selectedConfig;
      if (scenario.priority === 'speed') {
        selectedConfig = costTiers.budget[0] || configs[0];
      } else if (scenario.priority === 'quality') {
        selectedConfig = costTiers.premium[0] || configs[configs.length - 1];
      } else {
        selectedConfig = costTiers.standard[0] || configs[Math.floor(configs.length / 2)];
      }

      if (selectedConfig) {
        const pricing = selectedConfig.test_results?.pricing || { input: 0, output: 0 };
        const cost = (scenario.tokens * (pricing.input + pricing.output) / 2) / 1000000;
        const simulatedLatency = Math.floor(Math.random() * scenario.acceptableLatency * 0.8) + 1000;
        
        console.log(chalk.green(`\n   âœ… Selected: ${selectedConfig.model}`));
        console.log(chalk.gray(`      Provider: ${selectedConfig.provider}`));
        console.log(chalk.gray(`      Cost for scenario: $${cost.toFixed(6)}`));
        console.log(chalk.gray(`      Simulated latency: ${simulatedLatency}ms`));
        console.log(chalk.gray(`      Within SLA: ${simulatedLatency < scenario.acceptableLatency ? 'Yes' : 'No'}`));

        benchmarkResults.push({
          scenario: scenario.name,
          model: selectedConfig.model,
          cost,
          latency: simulatedLatency,
          withinSLA: simulatedLatency < scenario.acceptableLatency,
          tokensPerSecond: Math.floor(scenario.tokens / (simulatedLatency / 1000))
        });
      }
    }

    // Performance comparison
    console.log(chalk.yellow('\nðŸ“Š Performance Comparison:'));
    
    // Cost efficiency ranking
    const sortedByCost = benchmarkResults.sort((a, b) => a.cost - b.cost);
    console.log(chalk.blue('\nðŸ’° Cost Efficiency:'));
    sortedByCost.forEach((result, index) => {
      console.log(chalk.gray(`   ${index + 1}. ${result.scenario}: $${result.cost.toFixed(6)} (${result.model})`));
    });

    // Speed ranking
    const sortedBySpeed = benchmarkResults.sort((a, b) => a.latency - b.latency);
    console.log(chalk.blue('\nâš¡ Speed Ranking:'));
    sortedBySpeed.forEach((result, index) => {
      console.log(chalk.gray(`   ${index + 1}. ${result.scenario}: ${result.latency}ms (${result.model})`));
    });

    // Throughput analysis
    console.log(chalk.blue('\nðŸ“ˆ Throughput Analysis:'));
    benchmarkResults.forEach(result => {
      console.log(chalk.gray(`   ${result.scenario}: ${result.tokensPerSecond} tokens/sec`));
    });

    // Quality vs Cost trade-off
    console.log(chalk.yellow('\nðŸŽ¯ Quality vs Cost Analysis:'));
    const avgCostBudget = costTiers.budget.length > 0 ? 
      costTiers.budget.reduce((sum, c) => sum + ((c.test_results?.pricing?.input || 0) + (c.test_results?.pricing?.output || 0)) / 2, 0) / costTiers.budget.length : 0;
    const avgCostPremium = costTiers.premium.length > 0 ?
      costTiers.premium.reduce((sum, c) => sum + ((c.test_results?.pricing?.input || 0) + (c.test_results?.pricing?.output || 0)) / 2, 0) / costTiers.premium.length : 0;

    if (avgCostBudget > 0 && avgCostPremium > 0) {
      const costMultiplier = avgCostPremium / avgCostBudget;
      console.log(chalk.gray(`   Premium models cost ${costMultiplier.toFixed(1)}x more than budget models`));
      console.log(chalk.gray(`   Budget tier average: $${avgCostBudget.toFixed(2)}/1M tokens`));
      console.log(chalk.gray(`   Premium tier average: $${avgCostPremium.toFixed(2)}/1M tokens`));
    }

    // Recommendations
    console.log(chalk.blue('\nðŸ’¡ Recommendations:'));
    console.log(chalk.gray('   1. Use budget models (deepseek) for routine scans'));
    console.log(chalk.gray('   2. Reserve premium models (gpt-4o) for complex analyses'));
    console.log(chalk.gray('   3. Consider latency requirements when selecting models'));
    console.log(chalk.gray('   4. Batch similar requests to optimize costs'));
    console.log(chalk.gray('   5. Monitor token usage to prevent cost overruns'));

    // Final summary
    console.log(chalk.green('\nâœ… Performance Benchmarking Complete!'));
    console.log(chalk.gray('\nKey findings:'));
    console.log(chalk.gray(`   - Most cost-effective: ${sortedByCost[0].model} ($${sortedByCost[0].cost.toFixed(6)})`));
    console.log(chalk.gray(`   - Fastest response: ${sortedBySpeed[0].model} (${sortedBySpeed[0].latency}ms)`));
    console.log(chalk.gray(`   - All scenarios met SLA: ${benchmarkResults.every(r => r.withinSLA) ? 'Yes' : 'No'}`));

    return true;
  } catch (error) {
    console.log(chalk.red('\nâŒ Benchmark failed'));
    console.log(chalk.red(`Error: ${error.message}`));
    return false;
  }
}

// Run the benchmark
if (require.main === module) {
  runPerformanceBenchmark()
    .then((success) => {
      if (success) {
        console.log(chalk.green.bold('\nâœ… Phase 4.4 Performance Benchmarking completed!'));
      } else {
        console.log(chalk.red.bold('\nâŒ Phase 4.4 Performance Benchmarking failed!'));
      }
      process.exit(success ? 0 : 1);
    })
    .catch(error => {
      console.error(chalk.red('Fatal error:'), error);
      process.exit(1);
    });
}

module.exports = { runPerformanceBenchmark };