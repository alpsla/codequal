/**
 * RESEARCHER Agent - Continuously researches and updates optimal AI model configurations
 * 
 * This agent:
 * 1. Researches current AI models from all major providers
 * 2. Analyzes pricing, performance, and capabilities
 * 3. Generates optimal configurations for all agent roles, languages, and repository sizes
 * 4. Updates the CANONICAL_MODEL_VERSIONS collection dynamically
 * 5. Runs on scheduled intervals to keep configurations current
 */

import { AuthenticatedUser } from '../../../core/src/types';
import { Logger, createLogger } from '../../../core/src/utils';
import { 
  ModelVersionSync, 
  ModelVersionInfo, 
  CANONICAL_MODEL_VERSIONS,
  ModelCapabilities,
  ModelPricing,
  ModelTier,
  RepositoryContext,
  RepositorySizeCategory 
} from '../../../core/src/services/model-selection/ModelVersionSync';
import { RESEARCH_PROMPTS } from './research-prompts';

/**
 * Research configuration for the RESEARCHER agent
 */
export interface ResearchConfig {
  /**
   * Whether to include experimental/beta models
   */
  includeExperimental: boolean;
  
  /**
   * Maximum cost per 1M tokens for consideration
   */
  maxCostPerMillion: number;
  
  /**
   * Minimum performance threshold (1-10 scale)
   */
  minPerformanceThreshold: number;
  
  /**
   * Whether to prioritize cost over performance
   */
  prioritizeCost: boolean;
  
  /**
   * Model providers to research
   */
  providers: string[];
  
  /**
   * Research depth level
   */
  researchDepth: 'quick' | 'comprehensive' | 'deep' | 'dynamic_discovery';
}

/**
 * Persistent cache configuration for the researcher model
 */
export interface ResearcherCache {
  /**
   * Current researcher model being used
   */
  currentModel: ModelVersionInfo;
  
  /**
   * When the template was cached
   */
  templateCachedAt: Date;
  
  /**
   * Template ID for referencing cached content
   */
  templateId: string;
  
  /**
   * Session ID for conversation continuity
   */
  sessionId: string;
  
  /**
   * Total requests made using this cached researcher
   */
  requestCount: number;
  
  /**
   * Whether template is currently cached and active
   */
  isActive: boolean;
  
  /**
   * No expiration - cache until explicit upgrade
   */
  expiresAt: null;
  
  /**
   * Database configuration ID for synchronization
   */
  dbConfigId?: string;
}

/**
 * Meta-research result for evaluating the researcher itself
 */
export interface MetaResearchResult {
  /**
   * Current researcher model evaluation
   */
  currentModel: {
    provider: string;
    model: string;
    researchScore: number;
    strengths: string[];
    weaknesses: string[];
  };
  
  /**
   * Recommendation for researcher upgrade
   */
  recommendation: {
    shouldUpgrade: boolean;
    primary?: {
      provider: string;
      model: string;
      version: string;
      researchScore: number;
      whyBetterForResearch: string;
      costImplication: string;
    };
    fallback?: {
      provider: string;
      model: string;
      researchScore: number;
      whyFallback: string;
    };
  };
  
  /**
   * Upgrade recommendation details
   */
  upgradeRecommendation: {
    urgency: 'high' | 'medium' | 'low';
    reasoning: string;
    migrationEffort: string;
    expectedImprovement: string;
  };
  
  /**
   * When this meta-research was conducted
   */
  researchedAt: Date;
  
  /**
   * Confidence in the recommendation
   */
  confidence: number;
}

/**
 * Research result for a specific model
 */
export interface ModelResearchResult {
  /**
   * Model information discovered
   */
  model: ModelVersionInfo;
  
  /**
   * Confidence in this information (0-1)
   */
  confidence: number;
  
  /**
   * Source of the information
   */
  source: string;
  
  /**
   * Timestamp of research
   */
  researchedAt: Date;
  
  /**
   * Cost-effectiveness score (0-10)
   */
  costEffectivenessScore: number;
  
  /**
   * Recommended use cases
   */
  recommendedUseCases: string[];
}

/**
 * Configuration update recommendation
 */
export interface ConfigurationUpdate {
  /**
   * Context this update applies to
   */
  context: {
    language: string;
    sizeCategory: RepositorySizeCategory;
    agentRole: string;
  };
  
  /**
   * Current model configuration
   */
  currentModel: ModelVersionInfo | null;
  
  /**
   * Recommended new model
   */
  recommendedModel: ModelVersionInfo;
  
  /**
   * Reason for the recommendation
   */
  reason: string;
  
  /**
   * Expected improvement metrics
   */
  expectedImprovement: {
    costSaving?: number; // Percentage cost saving
    performanceGain?: number; // Performance improvement
    reliabilityGain?: number; // Reliability improvement
  };
  
  /**
   * Priority of this update (1-10)
   */
  priority: number;
}

/**
 * RESEARCHER Agent implementation
 */
export class ResearcherAgent {
  private logger: Logger;
  private modelVersionSync: ModelVersionSync;
  private researchConfig: ResearchConfig;
  private researcherCache: ResearcherCache | null = null;
  
  constructor(
    private authenticatedUser: AuthenticatedUser,
    config?: Partial<ResearchConfig>
  ) {
    this.logger = createLogger('ResearcherAgent');
    this.modelVersionSync = new ModelVersionSync(this.logger);
    
    // Default research configuration
    this.researchConfig = {
      includeExperimental: false,
      maxCostPerMillion: 100.00, // $100 per 1M tokens max
      minPerformanceThreshold: 6.0, // Minimum 6/10 performance
      prioritizeCost: false, // Quality-first focused (changed default)
      providers: ['openai', 'anthropic', 'google', 'deepseek', 'openrouter', 'meta', 'mistral', 'cohere'],
      researchDepth: 'dynamic_discovery', // Use dynamic discovery by default
      ...config
    };
    
    // Initialize persistent cache
    this.initializePersistentCache();
    
    this.logger.info('RESEARCHER Agent initialized', {
      userId: this.authenticatedUser.id,
      config: this.researchConfig,
      cacheStatus: this.researcherCache ? 'active' : 'initializing'
    });
  }
  
  /**
   * Main research and update workflow
   * This is the primary method that orchestrates the entire research process
   */
  async conductResearchAndUpdate(): Promise<{
    researchResults: ModelResearchResult[];
    configurationUpdates: ConfigurationUpdate[];
    summary: {
      modelsResearched: number;
      configurationsUpdated: number;
      totalCostSavings: number;
      performanceImprovements: number;
    };
  }> {
    this.logger.info('üî¨ Starting comprehensive model research and configuration update');
    
    try {
      // Step 1: Research current AI models from all providers
      this.logger.info('üìä Step 1: Researching current AI models...');
      const researchResults = await this.researchCurrentModels();
      
      // Step 2: Analyze and score models
      this.logger.info('üéØ Step 2: Analyzing and scoring models...');
      const scoredModels = await this.analyzeAndScoreModels(researchResults);
      
      // Step 3: Generate configuration recommendations
      this.logger.info('‚öôÔ∏è Step 3: Generating configuration recommendations...');
      const configurationUpdates = await this.generateConfigurationRecommendations(scoredModels);
      
      // Step 4: Apply high-priority updates
      this.logger.info('üöÄ Step 4: Applying configuration updates...');
      const appliedUpdates = await this.applyConfigurationUpdates(configurationUpdates);
      
      // Step 5: Generate summary
      const summary = this.generateResearchSummary(researchResults, appliedUpdates);
      
      this.logger.info('‚úÖ Research and update completed successfully', summary);
      
      return {
        researchResults,
        configurationUpdates: appliedUpdates,
        summary
      };
      
    } catch (error) {
      this.logger.error('‚ùå Research and update failed', { error });
      throw error;
    }
  }
  
  /**
   * Research current AI models using dynamic discovery - no hardcoded limitations
   */
  private async researchCurrentModels(): Promise<ModelResearchResult[]> {
    const results: ModelResearchResult[] = [];
    
    // Use the configured RESEARCHER model to discover new models
    const researcherContext: RepositoryContext = {
      language: 'researcher',
      sizeCategory: 'medium',
      tags: ['model_research', 'dynamic_discovery']
    };
    
    const researcherModel = this.modelVersionSync.findOptimalModel(researcherContext);
    
    if (!researcherModel) {
      throw new Error('RESEARCHER model not found in configuration');
    }
    
    this.logger.info('ü§ñ Using RESEARCHER model for dynamic discovery', {
      provider: researcherModel.provider,
      model: researcherModel.model
    });
    
    // First, do dynamic discovery to find ALL current models
    const discoveredModels = await this.dynamicModelDiscovery(researcherModel);
    
    // Then research each discovered provider
    for (const provider of this.extractProvidersFromDiscovery(discoveredModels)) {
      const providerResults = await this.researchProviderModels(provider, researcherModel);
      results.push(...providerResults);
    }
    
    // Add any models found through dynamic discovery
    results.push(...discoveredModels);
    
    return results;
  }

  /**
   * Perform dynamic model discovery - find models not in hardcoded lists
   */
  private async dynamicModelDiscovery(researcherModel: ModelVersionInfo): Promise<ModelResearchResult[]> {
    this.logger.info('üîç Starting dynamic model discovery using DYNAMIC_MODEL_DISCOVERY prompt');
    
    // In a full implementation, this would use the RESEARCH_PROMPTS.DYNAMIC_MODEL_DISCOVERY
    // to instruct the RESEARCHER model to:
    // 1. Search the web for latest AI model announcements
    // 2. Query provider APIs for current model listings
    // 3. Search GitHub for new open-source models
    // 4. Check tech news and research papers
    
    const discoveryPrompt = RESEARCH_PROMPTS.DYNAMIC_MODEL_DISCOVERY;
    
    this.logger.info('üìã Using dynamic discovery prompt', {
      promptLength: discoveryPrompt.length,
      researcherModel: `${researcherModel.provider}/${researcherModel.model}`,
      note: 'Prompt instructs to find ALL available models, not just hardcoded ones'
    });
    
    // For now, simulate what dynamic discovery might find
    // In production, this would be actual AI model calls using the discovery prompt
    const potentialNewModels = [
      // Example of models that might be discovered dynamically
      'claude-4-opus', 'gpt-5-preview', 'gemini-3-ultra', 'llama-3.5-code',
      'deepseek-v3', 'cohere-command-r-plus', 'mistral-large-2', 'qwen-coder-plus',
      'phi-4', 'codestral-mamba', 'wizardcoder-34b-v1.1', 'starcoder2-15b'
    ];
    
    this.logger.info('üéØ Dynamic discovery simulation - would find these models', {
      models: potentialNewModels,
      actualImplementation: 'Would use RESEARCHER model with DYNAMIC_MODEL_DISCOVERY prompt',
      methodology: 'Web search + API queries + GitHub scanning + tech news'
    });
    
    // In production, parse the AI response and convert to ModelResearchResult[]
    const discoveredModels: ModelResearchResult[] = [];
    
    return discoveredModels;
  }

  /**
   * Extract unique providers from discovery results
   */
  private extractProvidersFromDiscovery(discoveredModels: ModelResearchResult[]): string[] {
    const discoveredProviders = new Set(discoveredModels.map(m => m.model.provider));
    // Combine with configured providers but prioritize discovery results
    return [...discoveredProviders, ...this.researchConfig.providers];
  }
  
  /**
   * Research models from a specific provider
   */
  private async researchProviderModels(
    provider: string, 
    researcherModel: ModelVersionInfo
  ): Promise<ModelResearchResult[]> {
    const results: ModelResearchResult[] = [];
    
    // Simulate research using the RESEARCHER model
    // In production, this would make actual API calls to research current models
    
    switch (provider) {
      case 'openai':
        results.push(...await this.getOpenAIModels());
        break;
      case 'anthropic':
        results.push(...await this.getAnthropicModels());
        break;
      case 'google':
        results.push(...await this.getGoogleModels());
        break;
      case 'deepseek':
        results.push(...await this.getDeepSeekModels());
        break;
      case 'openrouter':
        results.push(...await this.getOpenRouterModels());
        break;
      default:
        this.logger.warn(`Unknown provider: ${provider}`);
    }
    
    // Add research metadata
    return results.map(result => ({
      ...result,
      researchedAt: new Date(),
      source: `RESEARCHER-${researcherModel.model}`,
      confidence: 0.85 // High confidence in known models
    }));
  }
  
  /**
   * Get latest OpenAI models with updated pricing and capabilities
   */
  private async getOpenAIModels(): Promise<ModelResearchResult[]> {
    // Research shows these are the current optimal OpenAI models
    return [
      {
        model: {
          provider: 'openai',
          model: 'gpt-4o',
          versionId: 'gpt-4o-20250602',
          releaseDate: '2025-06-02',
          description: 'OpenAI GPT-4o - Latest version with improved efficiency',
          capabilities: {
            codeQuality: 8.7,
            speed: 9.1,
            contextWindow: 128000,
            reasoning: 8.8,
            detailLevel: 8.2
          },
          pricing: {
            input: 2.50, // Updated pricing as of June 2025
            output: 10.00
          },
          tier: ModelTier.PREMIUM,
          preferredFor: ['small_repositories', 'quick_analysis', 'typescript', 'javascript']
        },
        confidence: 0.9,
        source: 'OpenAI API Research',
        researchedAt: new Date(),
        costEffectivenessScore: 8.2,
        recommendedUseCases: ['quick analysis', 'small repos', 'frontend development']
      },
      {
        model: {
          provider: 'openai',
          model: 'gpt-4o-mini',
          versionId: 'gpt-4o-mini-20250602',
          releaseDate: '2025-06-02',
          description: 'OpenAI GPT-4o Mini - Cost-optimized version',
          capabilities: {
            codeQuality: 7.8,
            speed: 9.5,
            contextWindow: 128000,
            reasoning: 7.5,
            detailLevel: 7.2
          },
          pricing: {
            input: 0.15,
            output: 0.60
          },
          tier: ModelTier.STANDARD,
          preferredFor: ['cost_optimization', 'quick_analysis', 'educational']
        },
        confidence: 0.9,
        source: 'OpenAI API Research',
        researchedAt: new Date(),
        costEffectivenessScore: 9.4,
        recommendedUseCases: ['cost-sensitive analysis', 'educational content', 'simple tasks']
      }
    ];
  }
  
  /**
   * Get latest Anthropic models
   */
  private async getAnthropicModels(): Promise<ModelResearchResult[]> {
    return [
      {
        model: {
          provider: 'anthropic',
          model: 'claude-3-5-sonnet',
          versionId: 'claude-3-5-sonnet-20250602',
          releaseDate: '2025-06-02',
          description: 'Anthropic Claude 3.5 Sonnet - Latest reasoning-optimized model',
          capabilities: {
            codeQuality: 9.5,
            speed: 7.8,
            contextWindow: 200000,
            reasoning: 9.7,
            detailLevel: 9.4
          },
          pricing: {
            input: 3.00,
            output: 15.00
          },
          tier: ModelTier.PREMIUM,
          preferredFor: ['large_repositories', 'detailed_analysis', 'architecture', 'security']
        },
        confidence: 0.9,
        source: 'Anthropic API Research',
        researchedAt: new Date(),
        costEffectivenessScore: 8.8,
        recommendedUseCases: ['complex analysis', 'large codebases', 'architectural decisions']
      }
    ];
  }
  
  /**
   * Get latest Google models
   */
  private async getGoogleModels(): Promise<ModelResearchResult[]> {
    return [
      {
        model: {
          provider: 'google',
          model: 'gemini-2.5-flash',
          versionId: 'gemini-2.5-flash-20250602',
          releaseDate: '2025-06-02',
          description: 'Google Gemini 2.5 Flash - Optimized for speed and cost',
          capabilities: {
            codeQuality: 8.5,
            speed: 9.2,
            contextWindow: 100000,
            reasoning: 8.8,
            detailLevel: 8.0
          },
          pricing: {
            input: 0.075,
            output: 0.30
          },
          tier: ModelTier.STANDARD,
          preferredFor: ['researcher', 'cost_optimization', 'scheduled_tasks']
        },
        confidence: 0.9,
        source: 'Google AI Research',
        researchedAt: new Date(),
        costEffectivenessScore: 9.6,
        recommendedUseCases: ['research tasks', 'frequent operations', 'cost optimization']
      },
      {
        model: {
          provider: 'google',
          model: 'gemini-2.5-pro',
          versionId: 'gemini-2.5-pro-20250602',
          releaseDate: '2025-06-02',
          description: 'Google Gemini 2.5 Pro - High-performance model',
          capabilities: {
            codeQuality: 9.1,
            speed: 8.0,
            contextWindow: 100000,
            reasoning: 9.2,
            detailLevel: 8.9
          },
          pricing: {
            input: 7.00,
            output: 21.00
          },
          tier: ModelTier.PREMIUM,
          preferredFor: ['performance', 'complex_analysis', 'medium_repositories']
        },
        confidence: 0.9,
        source: 'Google AI Research',
        researchedAt: new Date(),
        costEffectivenessScore: 8.1,
        recommendedUseCases: ['performance analysis', 'complex reasoning', 'medium repos']
      }
    ];
  }
  
  /**
   * Get latest DeepSeek models
   */
  private async getDeepSeekModels(): Promise<ModelResearchResult[]> {
    return [
      {
        model: {
          provider: 'deepseek',
          model: 'deepseek-coder-v2',
          versionId: 'deepseek-coder-v2-20250602',
          releaseDate: '2025-06-02',
          description: 'DeepSeek Coder V2 - Enhanced code understanding',
          capabilities: {
            codeQuality: 9.0,
            speed: 8.2,
            contextWindow: 64000,
            reasoning: 8.5,
            detailLevel: 8.7
          },
          pricing: {
            input: 0.14,
            output: 0.28
          },
          tier: ModelTier.STANDARD,
          preferredFor: ['code_quality', 'medium_repositories', 'java', 'python', 'cpp']
        },
        confidence: 0.85,
        source: 'DeepSeek Research',
        researchedAt: new Date(),
        costEffectivenessScore: 9.2,
        recommendedUseCases: ['code analysis', 'backend development', 'systems programming']
      }
    ];
  }
  
  /**
   * Get latest OpenRouter models
   */
  private async getOpenRouterModels(): Promise<ModelResearchResult[]> {
    // OpenRouter provides access to multiple providers - research their latest offerings
    return [
      {
        model: {
          provider: 'openrouter',
          model: 'anthropic/claude-3-5-sonnet',
          versionId: 'claude-3-5-sonnet-20250602',
          releaseDate: '2025-06-02',
          description: 'Anthropic Claude 3.5 Sonnet via OpenRouter',
          capabilities: {
            codeQuality: 9.3,
            speed: 7.6,
            contextWindow: 200000,
            reasoning: 9.5,
            detailLevel: 9.2
          },
          pricing: {
            input: 3.60, // OpenRouter markup
            output: 18.00
          },
          tier: ModelTier.PREMIUM,
          preferredFor: ['large_repositories', 'detailed_analysis', 'openrouter_integration']
        },
        confidence: 0.85,
        source: 'OpenRouter API Research',
        researchedAt: new Date(),
        costEffectivenessScore: 8.3,
        recommendedUseCases: ['when direct API unavailable', 'unified access', 'enterprise']
      }
    ];
  }
  
  /**
   * Analyze and score models for cost-effectiveness and performance
   */
  private async analyzeAndScoreModels(
    researchResults: ModelResearchResult[]
  ): Promise<ModelResearchResult[]> {
    return researchResults.map(result => {
      // Calculate cost-effectiveness score
      const costEffectivenessScore = this.calculateCostEffectiveness(result.model);
      
      return {
        ...result,
        costEffectivenessScore
      };
    }).sort((a, b) => b.costEffectivenessScore - a.costEffectivenessScore);
  }
  
  /**
   * Calculate comprehensive model score balancing quality, recency, performance, and cost
   */
  private calculateCostEffectiveness(model: ModelVersionInfo): number {
    if (!model.capabilities || !model.pricing) return 0;
    
    const { capabilities, pricing } = model;
    
    // 1. Quality Score (40% weight) - Primary factor
    const qualityScore = (
      (capabilities.codeQuality || 0) * 0.35 +    // Most important for code analysis
      (capabilities.reasoning || 0) * 0.35 +      // Critical for complex analysis
      (capabilities.detailLevel || 0) * 0.20 +    // Important for actionable insights
      (capabilities.speed || 0) * 0.10            // Nice to have
    );
    
    // 2. Recency Score (25% weight) - Prefer latest models
    const recencyScore = this.calculateRecencyScore(model.releaseDate);
    
    // 3. Performance Score (20% weight) - Technical capabilities
    const performanceScore = (
      (capabilities.speed || 0) * 0.4 +
      this.normalizeContextWindow(capabilities.contextWindow || 0) * 0.3 +
      (capabilities.reasoning || 0) * 0.3
    );
    
    // 4. Cost Efficiency Score (15% weight) - Value for money
    const costScore = this.calculateCostScore(pricing);
    
    // Weighted combination based on research configuration
    let finalScore: number;
    
    if (this.researchConfig.prioritizeCost) {
      // Cost-conscious weighting
      finalScore = (
        qualityScore * 0.3 +
        recencyScore * 0.2 + 
        performanceScore * 0.2 +
        costScore * 0.3
      );
    } else {
      // Quality-first weighting (default)
      finalScore = (
        qualityScore * 0.4 +
        recencyScore * 0.25 +
        performanceScore * 0.2 +
        costScore * 0.15
      );
    }
    
    // Bonus for premium models with exceptional quality
    if (qualityScore > 9.0 && model.tier === ModelTier.PREMIUM) {
      finalScore += 0.3; // Quality bonus
    }
    
    // Bonus for very recent models (2025 releases)
    if (recencyScore > 9.0) {
      finalScore += 0.2; // Recency bonus
    }
    
    return Math.min(10, Math.max(0, finalScore));
  }
  
  /**
   * Calculate recency score based on release date
   */
  private calculateRecencyScore(releaseDate?: string): number {
    if (!releaseDate) return 5.0; // Default moderate score
    
    const release = new Date(releaseDate);
    const now = new Date();
    const monthsDiff = (now.getTime() - release.getTime()) / (1000 * 60 * 60 * 24 * 30);
    
    // Scoring: Latest models (0-3 months) = 10, decreasing over time
    if (monthsDiff <= 3) return 10.0;        // Brand new
    if (monthsDiff <= 6) return 9.0;         // Very recent  
    if (monthsDiff <= 12) return 8.0;        // Recent
    if (monthsDiff <= 18) return 6.0;        // Somewhat recent
    if (monthsDiff <= 24) return 4.0;        // Getting old
    return 2.0;                               // Old model
  }
  
  /**
   * Normalize context window to 0-10 scale
   */
  private normalizeContextWindow(contextWindow: number): number {
    // Context windows: 8k=3, 32k=5, 128k=8, 200k+=10
    if (contextWindow >= 200000) return 10.0;
    if (contextWindow >= 128000) return 8.0;
    if (contextWindow >= 64000) return 6.0;
    if (contextWindow >= 32000) return 5.0;
    if (contextWindow >= 16000) return 4.0;
    if (contextWindow >= 8000) return 3.0;
    return 2.0;
  }
  
  /**
   * Calculate cost score (higher score = better value)
   */
  private calculateCostScore(pricing: ModelPricing): number {
    const avgCost = (pricing.input + pricing.output) / 2;
    
    // Cost scoring: Very cheap=10, expensive=1
    // $0.1/M = 10, $1/M = 8, $5/M = 6, $15/M = 4, $50/M = 1
    if (avgCost <= 0.2) return 10.0;        // Ultra cheap (Gemini Flash)
    if (avgCost <= 1.0) return 9.0;         // Very cheap (DeepSeek)
    if (avgCost <= 3.0) return 8.0;         // Cheap (GPT-4o mini)
    if (avgCost <= 8.0) return 6.0;         // Moderate (GPT-4o)
    if (avgCost <= 15.0) return 4.0;        // Expensive (Claude)
    if (avgCost <= 30.0) return 2.0;        // Very expensive
    return 1.0;                              // Extremely expensive
  }
  
  /**
   * Generate configuration recommendations using role-specific cross-market research
   */
  private async generateConfigurationRecommendations(
    scoredModels: ModelResearchResult[]
  ): Promise<ConfigurationUpdate[]> {
    const updates: ConfigurationUpdate[] = [];
    
    // Define all contexts to generate configurations for
    const languages = ['javascript', 'typescript', 'python', 'java', 'ruby', 'go', 'rust', 'csharp', 'php', 'c', 'cpp'];
    const sizeCategories: RepositorySizeCategory[] = ['small', 'medium', 'large'];
    const agentRoles = ['security', 'performance', 'architecture', 'codeQuality', 'dependency', 'educational', 'orchestrator'];
    
    // Generate role-specific recommendations (in parallel for performance)
    const rolePromises = agentRoles.map(async (agentRole) => {
      return this.generateRoleSpecificRecommendations(agentRole, languages, sizeCategories);
    });
    
    const roleUpdates = await Promise.all(rolePromises);
    
    // Flatten all updates
    for (const roleUpdateList of roleUpdates) {
      updates.push(...roleUpdateList);
    }
    
    // Sort by priority (high priority updates first)
    return updates.sort((a, b) => b.priority - a.priority);
  }

  /**
   * Generate cross-market recommendations for a specific agent role
   */
  private async generateRoleSpecificRecommendations(
    agentRole: string,
    languages: string[],
    sizeCategories: RepositorySizeCategory[]
  ): Promise<ConfigurationUpdate[]> {
    this.logger.info(`üéØ Researching best cross-market model for ${agentRole} agent`);
    
    // Use role-specific research prompt
    const researchPrompt = this.getRoleSpecificPrompt(agentRole);
    
    // In production, this would use the RESEARCHER model to find the absolute best model
    // across ALL providers for this specific role
    const roleRecommendation = await this.crossMarketRoleResearch(agentRole, researchPrompt);
    
    const updates: ConfigurationUpdate[] = [];
    
    // Apply the best model recommendation to all relevant contexts for this role
    for (const language of languages) {
      for (const sizeCategory of sizeCategories) {
        const context = { language, sizeCategory, agentRole };
        
        // Check if this role-specific model is better than current configuration
        const update = await this.evaluateRoleSpecificUpdate(context, roleRecommendation);
        
        if (update) {
          updates.push(update);
        }
      }
    }
    
    return updates;
  }

  /**
   * Get the appropriate research prompt for a specific agent role
   */
  private getRoleSpecificPrompt(agentRole: string): string {
    // Get role-specific requirements
    const rolePrompt = this.getRoleSpecificRequirements(agentRole);
    
    // Combine with dynamic discovery - no hardcoded models
    const currentYear = new Date().getFullYear();
    const dynamicPrompt = RESEARCH_PROMPTS.DYNAMIC_MODEL_DISCOVERY
      .replace(/{CURRENT_YEAR}/g, currentYear.toString())
      .replace(/{AGENT_ROLE}/g, agentRole.toUpperCase())
      .replace(/{ROLE_REQUIREMENTS}/g, rolePrompt);
    
    return dynamicPrompt;
  }

  /**
   * Get specific requirements for each agent role
   */
  private getRoleSpecificRequirements(agentRole: string): string {
    switch (agentRole) {
      case 'security':
        return RESEARCH_PROMPTS.SECURITY_AGENT_RESEARCH;
      case 'performance':
        return RESEARCH_PROMPTS.PERFORMANCE_AGENT_RESEARCH;
      case 'architecture':
        return RESEARCH_PROMPTS.ARCHITECTURE_AGENT_RESEARCH;
      case 'codeQuality':
        return RESEARCH_PROMPTS.CODE_QUALITY_AGENT_RESEARCH;
      default:
        return RESEARCH_PROMPTS.AGENT_REQUIREMENT_RESEARCH;
    }
  }

  /**
   * Perform cross-market research for a specific agent role
   */
  private async crossMarketRoleResearch(
    agentRole: string, 
    researchPrompt: string
  ): Promise<{
    primary: ModelVersionInfo;
    fallback: ModelVersionInfo;
    confidence: number;
  }> {
    this.logger.info(`üîç Cross-market research for ${agentRole} agent`, {
      promptLength: researchPrompt.length,
      approach: 'Single best model across ALL providers'
    });
    
    // In production, this would:
    // 1. Use the RESEARCHER model with the role-specific prompt
    // 2. Research ALL available models across ALL providers
    // 3. Find the SINGLE BEST model for this specific role
    // 4. Return primary + fallback recommendations
    
    // Simulate cross-market research results
    const mockResults = this.simulateCrossMarketResearch(agentRole);
    
    return mockResults;
  }

  /**
   * Simulate what cross-market research would find for each role
   */
  private simulateCrossMarketResearch(agentRole: string): {
    primary: ModelVersionInfo;
    fallback: ModelVersionInfo;
    confidence: number;
  } {
    // This simulates what cross-market research would discover dynamically
    // In production, this would use DYNAMIC_MODEL_DISCOVERY to find current best models
    const roleOptimalModels = {
      security: {
        primary: 'anthropic/latest-claude-model',   // Would be discovered dynamically
        fallback: 'google/latest-cost-effective'    // Would be discovered dynamically
      },
      performance: {
        primary: 'deepseek/latest-coder-model',     // Would be discovered dynamically
        fallback: 'openai/latest-performance-model' // Would be discovered dynamically
      },
      architecture: {
        primary: 'anthropic/latest-claude-model',   // Would be discovered dynamically
        fallback: 'google/latest-pro-model'        // Would be discovered dynamically
      },
      codeQuality: {
        primary: 'deepseek/latest-coder-model',     // Would be discovered dynamically
        fallback: 'openai/latest-general-model'    // Would be discovered dynamically
      }
    };

    const recommendation = roleOptimalModels[agentRole] || roleOptimalModels.codeQuality;
    
    // Return mock model info - in production this would be real discovered models
    return {
      primary: {
        provider: recommendation.primary.split('/')[0],
        model: recommendation.primary.split('/')[1],
        versionId: `${recommendation.primary.split('/')[1]}-20250602`,
        releaseDate: '2025-06-02',
        capabilities: { codeQuality: 9.0, speed: 8.0, reasoning: 9.0, detailLevel: 8.5, contextWindow: 128000 },
        pricing: { input: 2.0, output: 8.0 },
        tier: ModelTier.PREMIUM,
        preferredFor: [agentRole]
      } as ModelVersionInfo,
      fallback: {
        provider: recommendation.fallback.split('/')[0],
        model: recommendation.fallback.split('/')[1],
        versionId: `${recommendation.fallback.split('/')[1]}-20250602`,
        releaseDate: '2025-06-02',
        capabilities: { codeQuality: 8.0, speed: 9.0, reasoning: 8.0, detailLevel: 8.0, contextWindow: 100000 },
        pricing: { input: 0.5, output: 2.0 },
        tier: ModelTier.STANDARD,
        preferredFor: [agentRole]
      } as ModelVersionInfo,
      confidence: 0.9
    };
  }

  /**
   * Evaluate if the role-specific model is better than current configuration
   */
  private async evaluateRoleSpecificUpdate(
    context: { language: string; sizeCategory: RepositorySizeCategory; agentRole: string },
    roleRecommendation: { primary: ModelVersionInfo; fallback: ModelVersionInfo; confidence: number }
  ): Promise<ConfigurationUpdate | null> {
    // Get current optimal model for this context
    const repositoryContext: RepositoryContext = {
      language: context.language,
      sizeCategory: context.sizeCategory,
      tags: [context.agentRole]
    };
    
    const currentModel = this.modelVersionSync.findOptimalModel(repositoryContext);
    
    // Check if the role-specific primary model is better
    const improvement = this.calculateImprovement(currentModel, roleRecommendation.primary);
    
    if (improvement.shouldUpdate) {
      return {
        context,
        currentModel,
        recommendedModel: roleRecommendation.primary,
        reason: `Cross-market research found better ${context.agentRole} model: ${improvement.reason}`,
        expectedImprovement: improvement.metrics,
        priority: this.calculateUpdatePriority(context, improvement)
      };
    }
    
    return null;
  }
  
  /**
   * Generate recommendation for a specific context
   */
  private async generateContextualRecommendation(
    context: { language: string; sizeCategory: RepositorySizeCategory; agentRole: string },
    scoredModels: ModelResearchResult[]
  ): Promise<ConfigurationUpdate | null> {
    // Get current optimal model for this context
    const repositoryContext: RepositoryContext = {
      language: context.language,
      sizeCategory: context.sizeCategory,
      tags: [context.agentRole]
    };
    
    const currentModel = this.modelVersionSync.findOptimalModel(repositoryContext);
    
    // Find best model from research results for this context
    const recommendedModel = this.findBestModelForContext(context, scoredModels);
    
    if (!recommendedModel) {
      return null;
    }
    
    // Check if an update is beneficial
    const improvement = this.calculateImprovement(currentModel, recommendedModel.model);
    
    if (improvement.shouldUpdate) {
      return {
        context,
        currentModel,
        recommendedModel: recommendedModel.model,
        reason: improvement.reason,
        expectedImprovement: improvement.metrics,
        priority: this.calculateUpdatePriority(context, improvement)
      };
    }
    
    return null;
  }
  
  /**
   * Find the best model for a specific context from research results
   */
  private findBestModelForContext(
    context: { language: string; sizeCategory: RepositorySizeCategory; agentRole: string },
    scoredModels: ModelResearchResult[]
  ): ModelResearchResult | null {
    // Filter models suitable for this context
    const suitableModels = scoredModels.filter(result => {
      const model = result.model;
      
      // Check if model is preferred for this language or context
      if (model.preferredFor) {
        const matchesLanguage = model.preferredFor.includes(context.language.toLowerCase());
        const matchesSize = model.preferredFor.includes(`${context.sizeCategory}_repositories`);
        const matchesRole = model.preferredFor.includes(context.agentRole.toLowerCase());
        const matchesGeneral = model.preferredFor.includes('general') || model.preferredFor.includes('default');
        
        return matchesLanguage || matchesSize || matchesRole || matchesGeneral;
      }
      
      return true; // Include if no specific preferences
    });
    
    if (suitableModels.length === 0) {
      return scoredModels[0] || null; // Fallback to best overall model
    }
    
    // Return the highest-scoring suitable model
    return suitableModels.sort((a, b) => b.costEffectivenessScore - a.costEffectivenessScore)[0];
  }
  
  /**
   * Calculate improvement metrics between current and recommended model
   */
  private calculateImprovement(
    currentModel: ModelVersionInfo | null,
    recommendedModel: ModelVersionInfo
  ): {
    shouldUpdate: boolean;
    reason: string;
    metrics: {
      costSaving?: number;
      performanceGain?: number;
      reliabilityGain?: number;
    };
  } {
    if (!currentModel) {
      return {
        shouldUpdate: true,
        reason: 'No current model configured',
        metrics: { performanceGain: 100 }
      };
    }
    
    const currentCost = currentModel.pricing ? (currentModel.pricing.input + currentModel.pricing.output) / 2 : 0;
    const recommendedCost = recommendedModel.pricing ? (recommendedModel.pricing.input + recommendedModel.pricing.output) / 2 : 0;
    
    const costSaving = currentCost > 0 ? ((currentCost - recommendedCost) / currentCost) * 100 : 0;
    
    const currentPerformance = this.calculateOverallPerformance(currentModel);
    const recommendedPerformance = this.calculateOverallPerformance(recommendedModel);
    const performanceGain = ((recommendedPerformance - currentPerformance) / currentPerformance) * 100;
    
    // Should update if significant cost saving OR performance gain
    const shouldUpdate = costSaving > 10 || performanceGain > 5 || !currentModel.capabilities;
    
    let reason = '';
    if (costSaving > 10) reason += `${costSaving.toFixed(1)}% cost saving. `;
    if (performanceGain > 5) reason += `${performanceGain.toFixed(1)}% performance gain. `;
    if (!currentModel.capabilities) reason += 'Missing capability data. ';
    
    return {
      shouldUpdate,
      reason: reason || 'Minor optimization available',
      metrics: {
        costSaving: costSaving > 0 ? costSaving : undefined,
        performanceGain: performanceGain > 0 ? performanceGain : undefined,
        reliabilityGain: shouldUpdate ? 5 : undefined
      }
    };
  }
  
  /**
   * Calculate overall performance score for a model
   */
  private calculateOverallPerformance(model: ModelVersionInfo): number {
    if (!model.capabilities) return 5; // Default moderate performance
    
    const { capabilities } = model;
    return (
      (capabilities.codeQuality || 5) * 0.3 +
      (capabilities.speed || 5) * 0.2 +
      (capabilities.reasoning || 5) * 0.3 +
      (capabilities.detailLevel || 5) * 0.2
    );
  }
  
  /**
   * Calculate priority for an update (1-10 scale)
   */
  private calculateUpdatePriority(
    context: { language: string; sizeCategory: RepositorySizeCategory; agentRole: string },
    improvement: any
  ): number {
    let priority = 5; // Base priority
    
    // High priority for popular languages
    if (['javascript', 'typescript', 'python', 'java'].includes(context.language)) {
      priority += 2;
    }
    
    // High priority for critical agent roles
    if (['security', 'performance'].includes(context.agentRole)) {
      priority += 1;
    }
    
    // High priority for large repositories (more impact)
    if (context.sizeCategory === 'large') {
      priority += 1;
    }
    
    // Priority based on cost savings
    if (improvement.metrics.costSaving && improvement.metrics.costSaving > 20) {
      priority += 2;
    }
    
    return Math.min(10, Math.max(1, priority));
  }
  
  /**
   * Apply configuration updates to the CANONICAL_MODEL_VERSIONS
   */
  private async applyConfigurationUpdates(
    updates: ConfigurationUpdate[]
  ): Promise<ConfigurationUpdate[]> {
    const appliedUpdates: ConfigurationUpdate[] = [];
    
    // Apply high-priority updates first
    const highPriorityUpdates = updates.filter(update => update.priority >= 7);
    
    for (const update of highPriorityUpdates) {
      try {
        const success = this.modelVersionSync.registerModel(update.recommendedModel);
        
        if (success) {
          appliedUpdates.push(update);
          this.logger.info('‚úÖ Applied configuration update', {
            context: update.context,
            model: `${update.recommendedModel.provider}/${update.recommendedModel.model}`,
            reason: update.reason
          });
        }
      } catch (error) {
        this.logger.error('‚ùå Failed to apply configuration update', {
          context: update.context,
          error
        });
      }
    }
    
    return appliedUpdates;
  }
  
  /**
   * Generate research summary
   */
  private generateResearchSummary(
    researchResults: ModelResearchResult[],
    appliedUpdates: ConfigurationUpdate[]
  ) {
    const totalCostSavings = appliedUpdates.reduce((total, update) => {
      return total + (update.expectedImprovement.costSaving || 0);
    }, 0);
    
    const performanceImprovements = appliedUpdates.filter(
      update => (update.expectedImprovement.performanceGain || 0) > 0
    ).length;
    
    return {
      modelsResearched: researchResults.length,
      configurationsUpdated: appliedUpdates.length,
      totalCostSavings: Math.round(totalCostSavings * 100) / 100,
      performanceImprovements
    };
  }
  
  /**
   * Schedule regular research updates for repository model configurations
   */
  async scheduleRegularUpdates(intervalHours = 24): Promise<void> {
    this.logger.info(`üïê Scheduling regular repository model updates every ${intervalHours} hours`);
    
    // Set up interval for regular research (repository context analysis)
    setInterval(async () => {
      try {
        this.logger.info('üîÑ Starting scheduled repository model research update');
        await this.conductResearchAndUpdate();
      } catch (error) {
        this.logger.error('‚ùå Scheduled research update failed', { error });
      }
    }, intervalHours * 60 * 60 * 1000);
  }
  
  /**
   * Schedule quarterly meta-research to evaluate the researcher itself
   * Based on realistic AI model release patterns (major releases every 3-4 months)
   */
  async scheduleQuarterlyMetaResearch(intervalDays = 90): Promise<void> {
    this.logger.info(`üóìÔ∏è Scheduling quarterly meta-research every ${intervalDays} days`);
    this.logger.info('üìä Meta-research evaluates if researcher model should be upgraded');
    
    // Set up quarterly interval for meta-research
    setInterval(async () => {
      try {
        this.logger.info('üî¨ Starting quarterly meta-research (researcher self-evaluation)');
        const metaResult = await this.conductMetaResearch();
        
        // Log meta-research results
        this.logger.info('üìã Meta-research completed', {
          currentModel: `${metaResult.currentModel.provider}/${metaResult.currentModel.model}`,
          shouldUpgrade: metaResult.recommendation.shouldUpgrade,
          urgency: metaResult.upgradeRecommendation.urgency,
          confidence: metaResult.confidence
        });
        
        // If high urgency upgrade recommended, log prominently
        if (metaResult.recommendation.shouldUpgrade && metaResult.upgradeRecommendation.urgency === 'high') {
          this.logger.warn('üö® HIGH PRIORITY: Meta-research recommends immediate researcher upgrade', {
            currentScore: metaResult.currentModel.researchScore,
            recommendedModel: metaResult.recommendation.primary?.model,
            expectedImprovement: metaResult.upgradeRecommendation.expectedImprovement
          });
        }
        
        // Store meta-research results for review
        await this.storeMetaResearchResult(metaResult);
        
      } catch (error) {
        this.logger.error('‚ùå Quarterly meta-research failed', { error });
      }
    }, intervalDays * 24 * 60 * 60 * 1000);
  }
  
  /**
   * Store meta-research results for manual review and decision-making
   */
  private async storeMetaResearchResult(metaResult: MetaResearchResult): Promise<void> {
    try {
      // In production, store in database or file system for review
      const resultSummary = {
        date: metaResult.researchedAt,
        currentModel: `${metaResult.currentModel.provider}/${metaResult.currentModel.model}`,
        currentScore: metaResult.currentModel.researchScore,
        shouldUpgrade: metaResult.recommendation.shouldUpgrade,
        urgency: metaResult.upgradeRecommendation.urgency,
        recommendedModel: metaResult.recommendation.primary?.model,
        reasoning: metaResult.upgradeRecommendation.reasoning,
        confidence: metaResult.confidence
      };
      
      this.logger.info('üíæ Stored meta-research result for review', resultSummary);
      
      // Store in environment variable for now (in production: database/file)
      const existingResults = JSON.parse(process.env.META_RESEARCH_HISTORY || '[]');
      existingResults.push(resultSummary);
      process.env.META_RESEARCH_HISTORY = JSON.stringify(existingResults.slice(-10)); // Keep last 10 results
      
    } catch (error) {
      this.logger.error('Failed to store meta-research result', { error });
    }
  }
  
  /**
   * Initialize persistent cache for the researcher model
   * Cache once and use until explicit upgrade - no expiration
   */
  private initializePersistentCache(): void {
    // Try to load existing cache first
    const existingCache = this.loadExistingCache();
    
    if (existingCache && existingCache.isActive) {
      this.researcherCache = existingCache;
      this.logger.info('üì¶ Loaded existing researcher cache', {
        model: `${existingCache.currentModel.provider}/${existingCache.currentModel.model}`,
        cachedSince: existingCache.templateCachedAt,
        requestCount: existingCache.requestCount,
        sessionId: existingCache.sessionId
      });
      return;
    }
    
    // Initialize new cache
    const researcherModel = this.findCurrentResearcherModel();
    
    if (researcherModel) {
      this.researcherCache = {
        currentModel: researcherModel,
        templateCachedAt: new Date(),
        templateId: 'RESEARCH_TEMPLATE_V1',
        sessionId: Date.now().toString(),
        requestCount: 0,
        isActive: true,
        expiresAt: null // No expiration!
      };
      
      this.saveCache();
      
      this.logger.info('üîÑ Initialized new persistent researcher cache', {
        model: `${researcherModel.provider}/${researcherModel.model}`,
        templateId: this.researcherCache.templateId,
        sessionId: this.researcherCache.sessionId,
        expiration: 'NEVER (until explicit upgrade)'
      });
    }
  }
  
  /**
   * Load existing cache from storage (persistent across restarts)
   */
  private loadExistingCache(): ResearcherCache | null {
    try {
      // In production, this would load from Redis, database, or file system
      // For now, simulate loading cached researcher
      const savedCache = process.env.RESEARCHER_CACHE;
      
      if (savedCache) {
        return JSON.parse(savedCache) as ResearcherCache;
      }
      
      return null;
    } catch (error) {
      this.logger.warn('Failed to load existing researcher cache', { error });
      return null;
    }
  }
  
  /**
   * Save cache to persistent storage
   */
  private saveCache(): void {
    try {
      if (this.researcherCache) {
        // In production, save to Redis, database, or file system
        process.env.RESEARCHER_CACHE = JSON.stringify(this.researcherCache);
        
        this.logger.debug('üíæ Saved researcher cache to persistent storage');
      }
    } catch (error) {
      this.logger.error('Failed to save researcher cache', { error });
    }
  }
  
  /**
   * Find current researcher model from configuration
   */
  private findCurrentResearcherModel(): ModelVersionInfo | null {
    const researcherContext: RepositoryContext = {
      language: 'researcher',
      sizeCategory: 'medium',
      tags: ['model_research', 'dynamic_discovery']
    };
    
    return this.modelVersionSync.findOptimalModel(researcherContext);
  }
  
  /**
   * Use cached researcher for context requests (with template reference)
   * This is where the token savings happen - only send context, not full template
   * Includes automatic cache-DB synchronization
   */
  async useResearcherForContext(
    language: string,
    sizeCategory: RepositorySizeCategory,
    agentRole: string,
    frameworks: string[],
    complexity: number
  ): Promise<{
    prompt: string;
    tokensUsed: number;
    templateReused: boolean;
  }> {
    // Always check cache-DB sync before using researcher
    if (!await this.isCacheSyncWithDB()) {
      this.logger.info('üîÑ Cache out of sync with DB, rebuilding cache');
      await this.syncCacheWithDB();
    }
    
    if (!this.researcherCache || !this.researcherCache.isActive) {
      throw new Error('Researcher cache not available after sync attempt');
    }
    
    // Increment usage counter
    this.researcherCache.requestCount++;
    this.saveCache();
    
    // Generate context-specific request that references cached template
    const contextRequest = this.generateContextRequest(
      language,
      sizeCategory, 
      agentRole,
      frameworks,
      complexity
    );
    
    this.logger.info('üîÑ Using cached researcher for context request', {
      context: `${language}/${frameworks.join('/')} ${sizeCategory} ${agentRole}`,
      sessionId: this.researcherCache.sessionId,
      requestCount: this.researcherCache.requestCount,
      templateId: this.researcherCache.templateId,
      tokensUsed: Math.round(contextRequest.length * 0.75),
      cacheSync: 'verified'
    });
    
    return {
      prompt: contextRequest,
      tokensUsed: Math.round(contextRequest.length * 0.75),
      templateReused: true
    };
  }
  
  /**
   * Check if cache is synchronized with database configuration
   */
  async isCacheSyncWithDB(): Promise<boolean> {
    try {
      const dbConfig = await this.loadDBConfig();
      
      if (!dbConfig || !this.researcherCache) {
        return false;
      }
      
      // Check timestamp sync (DB must not be newer than cache)
      const isTimestampSync = dbConfig.updatedAt <= this.researcherCache.templateCachedAt;
      
      // Check config ID sync
      const isConfigIdSync = dbConfig.id === this.researcherCache.dbConfigId;
      
      // Check cache is active
      const isCacheActive = this.researcherCache.isActive;
      
      this.logger.debug('Cache-DB sync check', {
        dbUpdated: dbConfig.updatedAt,
        cacheCreated: this.researcherCache.templateCachedAt,
        dbConfigId: dbConfig.id,
        cacheConfigId: this.researcherCache.dbConfigId,
        timestampSync: isTimestampSync,
        configIdSync: isConfigIdSync,
        cacheActive: isCacheActive,
        overallSync: isTimestampSync && isConfigIdSync && isCacheActive
      });
      
      return isTimestampSync && isConfigIdSync && isCacheActive;
      
    } catch (error) {
      this.logger.error('Failed to check cache-DB sync', { error });
      return false;
    }
  }
  
  /**
   * Synchronize cache with database configuration
   */
  async syncCacheWithDB(): Promise<void> {
    try {
      this.logger.info('üîÑ Syncing researcher cache with database');
      
      // Invalidate old cache
      if (this.researcherCache) {
        this.researcherCache.isActive = false;
        this.saveCache();
        this.logger.info('‚ùå Invalidated old cache');
      }
      
      // Load new config from database
      const dbConfig = await this.loadDBConfig();
      
      if (!dbConfig) {
        throw new Error('No researcher configuration found in database');
      }
      
      this.logger.info('üì• Loaded researcher config from DB', {
        provider: dbConfig.provider,
        model: dbConfig.model,
        version: dbConfig.version,
        updatedAt: dbConfig.updatedAt
      });
      
      // Build new cache
      this.researcherCache = {
        currentModel: {
          provider: dbConfig.provider,
          model: dbConfig.model,
          versionId: dbConfig.version,
          releaseDate: dbConfig.releaseDate || new Date().toISOString().split('T')[0],
          capabilities: dbConfig.capabilities || {
            codeQuality: 8.0,
            speed: 8.0,
            contextWindow: 128000,
            reasoning: 8.0,
            detailLevel: 8.0
          },
          pricing: dbConfig.pricing || { input: 1.0, output: 3.0 },
          tier: dbConfig.tier || ModelTier.STANDARD,
          preferredFor: ['researcher', 'analysis']
        },
        templateCachedAt: new Date(),
        templateId: this.generateNewTemplateId(),
        sessionId: this.generateNewSessionId(),
        requestCount: 0,
        isActive: true,
        expiresAt: null,
        dbConfigId: dbConfig.id
      };
      
      // Save new cache
      this.saveCache();
      
      this.logger.info('‚úÖ Cache rebuilt with new researcher', {
        provider: this.researcherCache.currentModel.provider,
        model: this.researcherCache.currentModel.model,
        sessionId: this.researcherCache.sessionId,
        templateId: this.researcherCache.templateId,
        dbConfigId: this.researcherCache.dbConfigId
      });
      
      // Re-cache template with new researcher model
      await this.cacheTemplateWithNewResearcher();
      
    } catch (error) {
      this.logger.error('Failed to sync cache with DB', { error });
      throw error;
    }
  }
  
  /**
   * Load researcher configuration from database
   */
  private async loadDBConfig(): Promise<{
    id: string;
    provider: string;
    model: string;
    version: string;
    updatedAt: Date;
    releaseDate?: string;
    capabilities?: ModelCapabilities;
    pricing?: ModelPricing;
    tier?: ModelTier;
  } | null> {
    try {
      // In production: load from actual database
      // For now, simulate with environment variable or default
      const configStr = process.env.RESEARCHER_DB_CONFIG;
      
      if (configStr) {
        const config = JSON.parse(configStr);
        return {
          ...config,
          updatedAt: new Date(config.updatedAt)
        };
      }
      
      // Default configuration if none exists
      return {
        id: 'default_researcher_1',
        provider: 'google',
        model: 'gemini-2.5-flash',
        version: 'gemini-2.5-flash-20250603',
        updatedAt: new Date('2025-06-01T09:00:00Z'),
        capabilities: {
          codeQuality: 8.5,
          speed: 9.2,
          contextWindow: 100000,
          reasoning: 8.8,
          detailLevel: 8.0
        },
        pricing: { input: 0.075, output: 0.30 },
        tier: ModelTier.STANDARD
      };
      
    } catch (error) {
      this.logger.error('Failed to load DB config', { error });
      return null;
    }
  }
  
  /**
   * Save researcher configuration to database
   */
  private async saveDBConfig(config: {
    provider: string;
    model: string;
    version: string;
    reason?: string;
    capabilities?: ModelCapabilities;
    pricing?: ModelPricing;
    tier?: ModelTier;
  }): Promise<string> {
    try {
      const dbConfig = {
        id: this.generateNewConfigId(),
        ...config,
        updatedAt: new Date(),
        createdAt: new Date()
      };
      
      // In production: save to actual database
      // For now, simulate with environment variable
      process.env.RESEARCHER_DB_CONFIG = JSON.stringify(dbConfig);
      
      this.logger.info('üíæ Saved researcher config to DB', {
        id: dbConfig.id,
        provider: dbConfig.provider,
        model: dbConfig.model,
        updatedAt: dbConfig.updatedAt
      });
      
      return dbConfig.id;
      
    } catch (error) {
      this.logger.error('Failed to save DB config', { error });
      throw error;
    }
  }
  
  /**
   * Re-cache template with new researcher model
   */
  private async cacheTemplateWithNewResearcher(): Promise<void> {
    try {
      this.logger.info('üìã Re-caching template with new researcher model');
      
      // In production: send base template to new researcher model and cache response
      // For now, simulate successful template caching
      await new Promise(resolve => setTimeout(resolve, 500));
      
      this.logger.info('‚úÖ Template successfully cached with new researcher');
      
    } catch (error) {
      this.logger.error('Failed to cache template with new researcher', { error });
      throw error;
    }
  }
  
  /**
   * Generate context-specific request that references cached template
   */
  private generateContextRequest(
    language: string,
    sizeCategory: RepositorySizeCategory,
    agentRole: string,
    frameworks: string[],
    complexity: number
  ): string {
    const currentYear = new Date().getFullYear();
    const roleRequirements = this.getRoleSpecificRequirements(agentRole);
    const evaluationCriteria = this.getEvaluationCriteria(agentRole);
    
    return `**RESEARCH REQUEST [Session: ${this.researcherCache?.sessionId}]**
Reference Template: [${this.researcherCache?.templateId}]

**CONTEXT PARAMETERS:**
- Language: ${language}
- Frameworks: ${frameworks.join(', ')}
- Repository Size: ${sizeCategory}
- Complexity: ${complexity}x
- Agent Role: ${agentRole.toUpperCase()}
- Year: ${currentYear}

**ROLE-SPECIFIC REQUIREMENTS:**
${roleRequirements}

**EVALUATION CRITERIA:**
${evaluationCriteria}

**SPECIFIC OBJECTIVE:**
Find optimal model for ${language}/${frameworks.join('/')} ${agentRole} analysis in ${sizeCategory} repositories with ${complexity}x complexity.

Apply the cached [${this.researcherCache?.templateId}] with these parameters.`;
  }
  
  /**
   * Get role-specific evaluation criteria
   */
  private getEvaluationCriteria(agentRole: string): string {
    const criteria = {
      security: `‚Ä¢ Threat Detection (30%): Real security issues
‚Ä¢ False Positives (20%): Noise minimization
‚Ä¢ Reasoning (25%): Explains WHY
‚Ä¢ Coverage (15%): Issue diversity
‚Ä¢ Cost (10%): Value for security`,
      
      performance: `‚Ä¢ Optimization Quality (35%): Actionable improvements
‚Ä¢ Technical Accuracy (25%): Correct analysis
‚Ä¢ Analysis Breadth (20%): CPU/memory/I/O coverage
‚Ä¢ Code Understanding (15%): Performance patterns
‚Ä¢ Cost (5%): Value efficiency`,
      
      architecture: `‚Ä¢ Design Understanding (40%): Complex systems
‚Ä¢ Pattern Recognition (25%): Anti-patterns
‚Ä¢ Strategic Thinking (20%): Long-term considerations
‚Ä¢ Technical Depth (10%): Tech stack knowledge
‚Ä¢ Communication (5%): Clear explanations`,
      
      codeQuality: `‚Ä¢ Code Understanding (35%): Structure comprehension
‚Ä¢ Best Practices (25%): Standards awareness
‚Ä¢ Refactoring (20%): Improvement suggestions
‚Ä¢ Language Expertise (15%): Multi-language skill
‚Ä¢ Detail Level (5%): Thorough analysis`
    };
    
    return criteria[agentRole] || criteria.performance;
  }
  
  /**
   * Conduct meta-research: Have the researcher research itself
   * This is separate from the context loop and evaluates research capabilities
   */
  async conductMetaResearch(): Promise<MetaResearchResult> {
    this.logger.info('üî¨ Starting meta-research: Researcher evaluating itself');
    
    if (!this.researcherCache || !this.researcherCache.isActive) {
      throw new Error('Researcher cache not initialized');
    }
    
    const currentModel = this.researcherCache.currentModel;
    const metaPrompt = this.generateMetaResearchPrompt(currentModel);
    
    this.logger.info('üìã Generated meta-research prompt', {
      currentModel: `${currentModel.provider}/${currentModel.model}`,
      promptLength: metaPrompt.length,
      purpose: 'Evaluate if researcher should be upgraded'
    });
    
    // In production, this would send the meta-research prompt to the current researcher
    // and get back a response about whether it should be replaced
    
    // Simulate meta-research response
    const metaResult = this.simulateMetaResearchResponse(currentModel);
    
    this.logger.info('‚úÖ Meta-research completed', {
      shouldUpgrade: metaResult.recommendation.shouldUpgrade,
      urgency: metaResult.upgradeRecommendation.urgency,
      confidence: metaResult.confidence
    });
    
    return metaResult;
  }
  
  /**
   * Generate meta-research prompt for the researcher to evaluate itself
   */
  private generateMetaResearchPrompt(currentModel: ModelVersionInfo): string {
    const currentYear = new Date().getFullYear();
    
    return `You are currently ${currentModel.provider} ${currentModel.model}, serving as the RESEARCHER agent for CodeQual. Your task is to research whether you should be replaced by a newer/better model for RESEARCH TASKS.

**CURRENT ROLE:** AI Model Researcher
**CURRENT MODEL:** ${currentModel.provider} ${currentModel.model} (${currentModel.versionId})
**TASK:** Find if there's a better model for conducting AI model research

**RESEARCH OBJECTIVE:**
Determine if there's a superior model for:
1. Researching and comparing AI models across providers
2. Evaluating model capabilities for specific use cases
3. Making cost/performance recommendations
4. Staying current with latest model releases
5. Providing context-specific model selections

**META-RESEARCH CRITERIA (Different from repository analysis):**
- **Research Capability** (30%): Ability to discover and evaluate new models
- **Market Knowledge** (25%): Understanding of AI model landscape
- **Analysis Quality** (20%): Depth of model comparison and reasoning
- **Cost Efficiency** (15%): Value for research tasks specifically
- **Currency** (10%): Access to latest model information

**CURRENT PERFORMANCE BASELINE:**
- Successfully researches models across OpenAI, Anthropic, Google, DeepSeek, Meta
- Provides context-specific recommendations for 300+ configurations
- Balances quality, performance, cost factors effectively
- Cost: ~${currentModel.pricing?.input || 0.075} per 1000 input tokens
- Speed: ${currentModel.capabilities?.speed || 8}/10 response times

**RESEARCH MISSION:**
Find models released in the last 3-6 months that might be superior for:
1. **Web Research**: Discovering latest AI models and releases
2. **Provider API Knowledge**: Understanding current model offerings
3. **Comparative Analysis**: Evaluating models against multiple criteria
4. **Context Synthesis**: Matching models to specific requirements
5. **Cost Analysis**: Balancing performance with token costs

**DISCOVERY FOCUS:**
- OpenAI: Any new GPT models with enhanced research capabilities?
- Anthropic: Claude 4 variants better for research tasks?
- Google: Newer Gemini models with superior analysis?
- Meta: Llama 3+ with research specialization?
- DeepSeek: R1 or other reasoning-focused models?
- Emerging: Any specialized research/analysis models?

**CRITICAL QUESTIONS:**
1. Are there models with better web search and discovery capabilities?
2. Do newer models have more current training data for AI landscape?
3. Is there better reasoning ability for model comparison tasks?
4. Are there cost-effective alternatives with superior research skills?
5. Should we prioritize research quality or cost efficiency?

**META-RESEARCH METHODOLOGY:**
1. Research latest releases across all major providers
2. Evaluate each for research-specific capabilities
3. Compare reasoning and analysis quality
4. Assess cost implications for research workload
5. Provide honest assessment including whether to replace yourself

Find the BEST model for AI model research tasks, even if it means recommending your own replacement.

**OUTPUT FORMAT:**
{
  "currentModel": {
    "provider": "${currentModel.provider}",
    "model": "${currentModel.model}",
    "researchScore": 8.5,
    "strengths": ["cost efficiency", "speed", "broad knowledge"],
    "weaknesses": ["potentially outdated training", "limited reasoning depth"]
  },
  "recommendation": {
    "shouldUpgrade": true/false,
    "primary": {
      "provider": "...",
      "model": "...",
      "version": "...",
      "researchScore": 9.2,
      "whyBetterForResearch": "Specific reasons why this model is superior for research tasks",
      "costImplication": "How cost changes for research operations"
    },
    "fallback": {
      "provider": "...",
      "model": "...",
      "researchScore": 8.8,
      "whyFallback": "Backup option for research tasks"
    }
  },
  "upgradeRecommendation": {
    "urgency": "high/medium/low",
    "reasoning": "Analysis of whether upgrade is needed now",
    "migrationEffort": "Effort required to switch researchers",
    "expectedImprovement": "Quantified benefits of upgrading"
  }
}`;
  }
  
  /**
   * Simulate meta-research response (in production, this would be actual AI response)
   */
  private simulateMetaResearchResponse(currentModel: ModelVersionInfo): MetaResearchResult {
    // Simulate what the current researcher might say about itself
    const isOlderModel = new Date(currentModel.releaseDate || '2025-01-01') < new Date('2025-05-01');
    const shouldUpgrade = isOlderModel || currentModel.capabilities?.reasoning < 9.0;
    
    return {
      currentModel: {
        provider: currentModel.provider,
        model: currentModel.model,
        researchScore: 8.5,
        strengths: ['cost efficiency', 'speed', 'good web search'],
        weaknesses: isOlderModel ? ['outdated training data', 'limited reasoning'] : ['minor reasoning limitations']
      },
      recommendation: {
        shouldUpgrade,
        primary: shouldUpgrade ? {
          provider: 'anthropic',
          model: 'claude-4-sonnet',
          version: 'claude-4-sonnet-20250603',
          researchScore: 9.4,
          whyBetterForResearch: 'Superior reasoning for model comparison, more current training data, better at synthesizing complex research',
          costImplication: '3x more expensive but significantly better research quality'
        } : undefined,
        fallback: shouldUpgrade ? {
          provider: 'openai',
          model: 'gpt-5-turbo',
          researchScore: 9.0,
          whyFallback: 'Good research capabilities with reasonable cost'
        } : undefined
      },
      upgradeRecommendation: {
        urgency: shouldUpgrade ? 'medium' : 'low',
        reasoning: shouldUpgrade 
          ? 'Current model adequate but newer models have superior reasoning for research tasks'
          : 'Current model performs well for research tasks',
        migrationEffort: 'Low - just update researcher configuration and re-cache template',
        expectedImprovement: shouldUpgrade ? '20-30% better research quality, more current model knowledge' : 'No significant improvement needed'
      },
      researchedAt: new Date(),
      confidence: 0.9
    };
  }
  
  /**
   * Explicitly upgrade the researcher model with atomic DB-cache coordination
   */
  async upgradeResearcher(
    newProvider: string,
    newModel: string,
    newVersion: string,
    reason: string,
    capabilities?: ModelCapabilities,
    pricing?: ModelPricing,
    tier?: ModelTier
  ): Promise<{
    success: boolean;
    oldModel: string;
    newModel: string;
    requiresRecaching: boolean;
    dbConfigId?: string;
  }> {
    if (!this.researcherCache) {
      throw new Error('No researcher cache to upgrade');
    }
    
    const oldModel = `${this.researcherCache.currentModel.provider}/${this.researcherCache.currentModel.model}`;
    const newModelId = `${newProvider}/${newModel}`;
    
    this.logger.info('üîÑ UPGRADING RESEARCHER', {
      from: oldModel,
      to: newModelId,
      version: newVersion,
      reason,
      requestsUsed: this.researcherCache.requestCount
    });
    
    try {
      // Step 1: Save new configuration to database first (atomic operation)
      const dbConfigId = await this.saveDBConfig({
        provider: newProvider,
        model: newModel,
        version: newVersion,
        reason,
        capabilities,
        pricing,
        tier
      });
      
      this.logger.info('‚úÖ DB configuration updated', { dbConfigId });
      
      // Step 2: Invalidate current cache
      this.researcherCache.isActive = false;
      this.saveCache();
      this.logger.info('‚ùå Cache invalidated');
      
      // Step 3: Cache will be rebuilt on next use via syncCacheWithDB()
      // This ensures lazy synchronization and consistency
      
      this.logger.info('‚úÖ RESEARCHER UPGRADE COMPLETED', {
        oldModel,
        newModel: newModelId,
        dbConfigId,
        cacheInvalidated: true,
        willRebuildOnNextUse: true
      });
      
      return {
        success: true,
        oldModel,
        newModel: newModelId,
        requiresRecaching: true,
        dbConfigId
      };
      
    } catch (error) {
      this.logger.error('‚ùå RESEARCHER UPGRADE FAILED', { 
        error: error instanceof Error ? error.message : String(error),
        oldModel, 
        newModel: newModelId 
      });
      
      // Restore old cache on failure
      if (this.researcherCache) {
        this.researcherCache.isActive = true;
        this.saveCache();
        this.logger.info('üîÑ Cache restored after upgrade failure');
      }
      
      return {
        success: false,
        oldModel,
        newModel: newModelId,
        requiresRecaching: false
      };
    }
  }
  
  /**
   * Find model information for upgrade
   */
  private async findModelInfo(
    provider: string,
    model: string,
    version: string
  ): Promise<ModelVersionInfo | null> {
    // In production, this would look up the model from research results
    // For now, create a basic model info structure
    return {
      provider,
      model,
      versionId: version,
      releaseDate: new Date().toISOString().split('T')[0],
      capabilities: {
        codeQuality: 9.0,
        speed: 8.0,
        contextWindow: 200000,
        reasoning: 9.5,
        detailLevel: 9.0
      },
      pricing: {
        input: 3.0,
        output: 15.0
      },
      tier: ModelTier.PREMIUM,
      preferredFor: ['researcher', 'analysis', 'reasoning']
    };
  }
  
  /**
   * Get cache statistics
   */
  getCacheStats(): {
    isActive: boolean;
    model: string;
    cachedSince: Date | null;
    requestCount: number;
    sessionId: string | null;
    templateId: string | null;
    dbConfigId: string | null;
    tokensSaved: number;
    isSyncWithDB?: boolean;
  } {
    if (!this.researcherCache) {
      return {
        isActive: false,
        model: 'None',
        cachedSince: null,
        requestCount: 0,
        sessionId: null,
        templateId: null,
        dbConfigId: null,
        tokensSaved: 0
      };
    }
    
    const templateTokens = 1301; // Base template size
    const tokensSaved = this.researcherCache.requestCount * templateTokens;
    
    return {
      isActive: this.researcherCache.isActive,
      model: `${this.researcherCache.currentModel.provider}/${this.researcherCache.currentModel.model}`,
      cachedSince: this.researcherCache.templateCachedAt,
      requestCount: this.researcherCache.requestCount,
      sessionId: this.researcherCache.sessionId,
      templateId: this.researcherCache.templateId,
      dbConfigId: this.researcherCache.dbConfigId,
      tokensSaved
    };
  }

  // Utility methods for ID generation
  private generateNewSessionId(): string {
    return `session_${Date.now()}_${Math.random().toString(36).substr(2, 8)}`;
  }

  private generateNewTemplateId(): string {
    return `RESEARCH_TEMPLATE_V${Date.now().toString().slice(-3)}`;
  }

  private generateNewConfigId(): string {
    return `researcher_config_${Date.now()}_${Math.random().toString(36).substr(2, 8)}`;
  }
}